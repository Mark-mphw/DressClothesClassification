{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare environment"
      ],
      "metadata": {
        "id": "fO9RJ5kJ9EjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets\n",
        "\n",
        "# Stratified split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# PyTorch TensorBoard support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "font = {'size' : 18}\n",
        "matplotlib.rc('font', **font)"
      ],
      "metadata": {
        "id": "sS9Rj0G98-tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare datasets"
      ],
      "metadata": {
        "id": "GXL0NsTo9B4P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWj21rIY5_jn"
      },
      "outputs": [],
      "source": [
        "# Download training data from open datasets.\n",
        "train_ds = datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=T.ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_ds = datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=T.ToTensor(),\n",
        ")\n",
        "\n",
        "# Preparing for validaion test\n",
        "train_indices, valid_indices, _, _ = train_test_split(\n",
        "    range(len(train_ds)),\n",
        "    train_ds.targets,\n",
        "    test_size=10000\n",
        ")\n",
        "valid_ds = Subset(train_ds, valid_indices)\n",
        "train_ds = Subset(train_ds, train_indices)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n",
        "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Class mapping\n",
        "classes = [\n",
        "    'Airplane',\n",
        "    'Automobile',\n",
        "    'Bird',\n",
        "    'Cat',\n",
        "    'Deer',\n",
        "    'Dog',\n",
        "    'Frog',\n",
        "    'Horse',\n",
        "    'Ship',\n",
        "    'Truck',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Report split sizes\n",
        "print(f'Training set: {len(train_ds)} examples')\n",
        "print(f'Validation set: {len(valid_ds)} examples')\n",
        "print(f'Test set: {len(test_ds)} examples')"
      ],
      "metadata": {
        "id": "EnUtlb3pfRtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is always a good idea to visualize the input and the label obtained from the DataLoader to make sure that the input data are as expected."
      ],
      "metadata": {
        "id": "lbgoCkUQvQqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch of data\n",
        "for batch in train_dl:\n",
        "    imgs, lbls = batch\n",
        "    break\n",
        "\n",
        "print(imgs.shape)\n",
        "print(lbls.shape)"
      ],
      "metadata": {
        "id": "xHh4GmDuvdGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(imgs), size=(1,)).item()\n",
        "    img, label = imgs[sample_idx], lbls[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(classes[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.numpy().transpose(1,2,0))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V6vguEdmwFil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a model"
      ],
      "metadata": {
        "id": "aZkzds3X88Xe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlF7qNi_5_jp"
      },
      "outputs": [],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        # out = # YOUR CODE HERE\n",
        "        return out\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have created a model, it is always a good idea to try to process the input data from the DataLoader to make sure that we can train the model."
      ],
      "metadata": {
        "id": "8Q4vj2-zx1n6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch of data\n",
        "for batch in train_dl:\n",
        "    imgs, lbls = batch\n",
        "    break\n",
        "\n",
        "preds = model(imgs)\n",
        "print(preds.shape)"
      ],
      "metadata": {
        "id": "JGWRQskQx-FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyebIUFG5_jq"
      },
      "source": [
        "# Train a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VDPtHxc5_jq"
      },
      "outputs": [],
      "source": [
        "# Specify loss\n",
        "# Note: Make sure to check the following when choosing the loss\n",
        "#   1. the label format of each loss (one-hot, ordinal, etc.)\n",
        "#   2. classification or regression\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify optimizer\n",
        "# e.g., SGD, Adam, RMSProp, etc.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "u28VQNdHwjoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XGl4Q8V5_jr"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
        "    # Size of the dataset\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    # Training mode\n",
        "    model.train()\n",
        "\n",
        "    # Keep track of the loss\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # We use enumerate to track the batch index\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        X, y = batch\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Zeros the optimizer's gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Prediction\n",
        "        pred = model(X)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()  # calculate the backward gradients\n",
        "        optimizer.step()  # adjust model's weights based on the observed gradients\n",
        "\n",
        "        # Keep track of the loss\n",
        "        running_loss += loss.item()\n",
        "        if (i+1) % 50 == 0:\n",
        "            last_loss = running_loss / 50\n",
        "            current = (i + 1) * len(X)\n",
        "            print(f\"loss: {last_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            running_loss = 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gtl9HK4O5_jr"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    # Test mode\n",
        "    model.eval()\n",
        "\n",
        "    # Predict on test set\n",
        "    loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            # Prediction\n",
        "            pred = model(X)\n",
        "            y_pred = pred.argmax(1)\n",
        "            # Compute loss\n",
        "            loss += loss_fn(pred, y).item()\n",
        "            # Correct predictions\n",
        "            correct += (y_pred == y).type(torch.float).sum().item()\n",
        "\n",
        "    # Average loss\n",
        "    loss /= num_batches\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = correct / size\n",
        "\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp0BCKNG5_jr"
      },
      "source": [
        "The training process is conducted over several iterations (*epochs*). During each epoch, the model learns\n",
        "parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the\n",
        "accuracy increase and the loss decrease with every epoch.\n",
        "We also keep track of the validation loss, and save the model that perform best on the validation set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyFw_NQA5_jr"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "best_vloss = 100000.\n",
        "hist = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': [],\n",
        "}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train_one_epoch(train_dl, model, loss_fn, optimizer)\n",
        "    train_loss, train_acc = test(train_dl, model, loss_fn)\n",
        "    val_loss, val_acc = test(valid_dl, model, loss_fn)\n",
        "\n",
        "    # History for learning curve\n",
        "    hist['train_loss'].append(train_loss)\n",
        "    hist['train_acc'].append(train_acc)\n",
        "    hist['val_loss'].append(val_loss)\n",
        "    hist['val_acc'].append(val_acc)\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if val_loss < best_vloss:\n",
        "        best_vloss = val_loss\n",
        "        torch.save(model.state_dict(), 'model_best_vloss.pth')\n",
        "        print('Saved best model to model_best_vloss.pth')\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "ax.plot(hist['train_loss'], label='train')\n",
        "ax.plot(hist['val_loss'], label='valid')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "ax.plot(hist['train_acc'], label='train')\n",
        "ax.plot(hist['val_acc'], label='valid')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.close('all')"
      ],
      "metadata": {
        "id": "fHu5oqzTujik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLzLQsvR5_jr"
      },
      "source": [
        "# Load Best Model on Validation Set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7Ny0BYO5_jr"
      },
      "outputs": [],
      "source": [
        "model_best = SimpleCNN().to(device)\n",
        "model_best.load_state_dict(torch.load(\"model_best_vloss.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate on the test set"
      ],
      "metadata": {
        "id": "xrGN_zf5sm64"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmz2wdJv5_jr"
      },
      "source": [
        "Predict on one example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-BRbrSn5_js"
      },
      "outputs": [],
      "source": [
        "model_best.eval()\n",
        "\n",
        "# Get one batch of data\n",
        "for batch in test_dl:\n",
        "    imgs, lbls = batch\n",
        "    break\n",
        "\n",
        "X, y = imgs[0:1].to(device), lbls[0:1].to(device)\n",
        "with torch.no_grad():\n",
        "    pred = model_best(X)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict on the whole test set."
      ],
      "metadata": {
        "id": "IEtEOeChsXlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the best model on the validation set\n",
        "test_loss, test_acc = test(test_dl, model_best, loss_fn)\n",
        "print(f\"Test: loss={test_loss:>8f}, acc={(100*test_acc):>0.1f}%\")"
      ],
      "metadata": {
        "id": "uvcTf5Ma97hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the latest model\n",
        "# Note: the latest model might be different from the best on validation set\n",
        "test_loss, test_acc = test(test_dl, model, loss_fn)\n",
        "print(f\"Test: loss={test_loss:>8f}, acc={(100*test_acc):>0.1f}%\")"
      ],
      "metadata": {
        "id": "gGemt3Dj7d-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "66o16eiCUZtv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}