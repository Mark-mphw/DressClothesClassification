{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Environment"
      ],
      "metadata": {
        "id": "4N5inrbSxgTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d73VS4iyEFfm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU or GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0L-H1_V1Hk4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "h9wHAMWAxeWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/data/thaifood'"
      ],
      "metadata": {
        "id": "qib-Kp75EPV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations\n",
        "# Train set: Data augmentation and normalization\n",
        "# Validation set: Just normalization (no randomness)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Dataset\n",
        "image_datasets = {}\n",
        "for k in data_transforms.keys():\n",
        "    image_datasets[k] = datasets.ImageFolder(\n",
        "        root=os.path.join(data_dir, k), \n",
        "        transform=data_transforms[k])\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 16\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=True, \n",
        "                                              num_workers=4)\n",
        "              for x in ['train', 'valid', 'test']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid', 'test']}\n",
        "class_names = image_datasets['train'].classes"
      ],
      "metadata": {
        "id": "aYX6BusTEa37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    # c, h, w --> h, w, c\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    # De-normalize\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "# Plot\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ],
      "metadata": {
        "id": "YySFS-SWFClK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning"
      ],
      "metadata": {
        "id": "tx0rEbP1ykAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'valid']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'valid' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best valid Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "w_DIjC19Ho0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['valid']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "metadata": {
        "id": "q0qz862-Iww9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    # Test mode\n",
        "    model.eval()\n",
        "\n",
        "    # Predict on test set\n",
        "    loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            # Prediction\n",
        "            pred = model(X)\n",
        "            y_pred = pred.argmax(1)\n",
        "            # Compute loss\n",
        "            loss += loss_fn(pred, y).item()\n",
        "            # Correct predictions\n",
        "            correct += (y_pred == y).type(torch.float).sum().item()\n",
        "\n",
        "    # Average loss\n",
        "    loss /= num_batches\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = correct / size\n",
        "\n",
        "    return loss, accuracy"
      ],
      "metadata": {
        "id": "_LgxALUtEX0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-trained Model as Fixed Feature Extractor"
      ],
      "metadata": {
        "id": "p_0PfK6bJdvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-trained weights\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "# Freeze pre-trained weights --> no update during training\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "bnVJHyKi0FGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let see the layers in the model\n",
        "print(model)"
      ],
      "metadata": {
        "id": "oVg5HvCJ0He9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the last layer of the resnet18\n",
        "num_ftrs = model.fc.in_features\n",
        "\n",
        "# Replace the last layer with the classification layer\n",
        "# Note: Parameters of newly constructed modules have requires_grad=True by default\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "# Move to desired device\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "ZW2oEEUwJVYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of trainable parameters\n",
        "summary(model, input_size=(3, 224, 224))"
      ],
      "metadata": {
        "id": "KpjZOVaF6iMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "jUZZ2CB52rUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = train_model(\n",
        "    model, criterion, \n",
        "    optimizer_conv, exp_lr_scheduler, \n",
        "    num_epochs=10)"
      ],
      "metadata": {
        "id": "eBNLtqobKcuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_model(model)"
      ],
      "metadata": {
        "id": "U3DEgSN4Kmma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = test(dataloaders['test'], model, criterion)\n",
        "print(f\"Test: loss={test_loss:>8f}, acc={(100*test_acc):>0.1f}%\")"
      ],
      "metadata": {
        "id": "byF99tMEEtv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Fine-tuning"
      ],
      "metadata": {
        "id": "978XBalxJbSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = list(model.children())\n",
        "for li, l in enumerate(layers):\n",
        "    print(f\"{li}: {l}\")"
      ],
      "metadata": {
        "id": "0Sek8-6y4jW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine how many layers to freeze\n",
        "fine_tune_at = 7\n",
        "ct = 0\n",
        "for child in model.children():\n",
        "    ct += 1\n",
        "    if ct < 7:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False\n",
        "    else:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = True"
      ],
      "metadata": {
        "id": "rUlui4Iz4hmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_size=(3, 224, 224))"
      ],
      "metadata": {
        "id": "LaBkM8n_6lWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# Note: we typically use a lower learning rate for model fine-tuning\n",
        "optimizer_ft = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "g-gTGkYGI7DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(\n",
        "    model, criterion, \n",
        "    optimizer_ft, exp_lr_scheduler,\n",
        "    num_epochs=20)"
      ],
      "metadata": {
        "id": "i8EYfLXjJKx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_model(model)"
      ],
      "metadata": {
        "id": "EYmuPxTeJSAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = test(dataloaders['test'], model, criterion)\n",
        "print(f\"Test: loss={test_loss:>8f}, acc={(100*test_acc):>0.1f}%\")"
      ],
      "metadata": {
        "id": "4I0lm788Fsvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Create your own custom dataset\n",
        "\n",
        "You need to create a new custom dataset for image classification whose datasets can be easily obtained from the Internet. Your dataset **MUST** have at least 4 classes. Then use this notebook to do transfer learning. You may want to adjust the code as appropriate:\n",
        "* Change the model (e.g., EfficientNet, Inception, etc.) [[link](https://pytorch.org/vision/stable/models.html#table-of-all-available-classification-weights)]\n",
        "* Change the training parameters (e.g., batch_size, learning rate, epoch, etc.)\n",
        "* Change the number of layer to be frozen.\n",
        "* and so on.\n",
        "\n",
        "Please make sure that you have enough training (>=15), valid (>=5) and test (>=5) examples for each class."
      ],
      "metadata": {
        "id": "bry9ssFx9P6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "A0ehVXHa-KKH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}