{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4N5inrbSxgTZ"
   },
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d73VS4iyEFfm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchsummary import summary\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0L-H1_V1Hk4w"
   },
   "outputs": [],
   "source": [
    "# CPU or GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9wHAMWAxeWv"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qib-Kp75EPV_"
   },
   "outputs": [],
   "source": [
    "data_dir = r'C:\\Users\\Mark.mphw\\OneDrive - Mahidol University\\iFile\\Learning Session\\SpecialTopicsInComputerScience\\Lecture 10\\Lab\\Data\\thaifood'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aYX6BusTEa37"
   },
   "outputs": [],
   "source": [
    "# Transformations\n",
    "# Train set: Data augmentation and normalization\n",
    "# Validation set: Just normalization (no randomness)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset\n",
    "image_datasets = {}\n",
    "for k in data_transforms.keys():\n",
    "    image_datasets[k] = datasets.ImageFolder(\n",
    "        root=os.path.join(data_dir, k), \n",
    "        transform=data_transforms[k])\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 16\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True, \n",
    "                                              num_workers=4)\n",
    "              for x in ['train', 'valid', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid', 'test']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YySFS-SWFClK"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    # c, h, w --> h, w, c\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # De-normalize\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# Plot\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx0rEbP1ykAt"
   },
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_DIjC19Ho0k"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best valid Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0qz862-Iww9"
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['valid']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LgxALUtEX0l"
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    # Test mode\n",
    "    model.eval()\n",
    "\n",
    "    # Predict on test set\n",
    "    loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Prediction\n",
    "            pred = model(X)\n",
    "            y_pred = pred.argmax(1)\n",
    "            # Compute loss\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # Correct predictions\n",
    "            correct += (y_pred == y).type(torch.float).sum().item()\n",
    "\n",
    "    # Average loss\n",
    "    loss /= num_batches\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = correct / size\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_0PfK6bJdvT"
   },
   "source": [
    "## Pre-trained Model as Fixed Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnVJHyKi0FGO"
   },
   "outputs": [],
   "source": [
    "# Pre-trained weights\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "# Freeze pre-trained weights --> no update during training\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVg5HvCJ0He9"
   },
   "outputs": [],
   "source": [
    "# Let see the layers in the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZW2oEEUwJVYW"
   },
   "outputs": [],
   "source": [
    "# Get the last layer of the resnet18\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# Replace the last layer with the classification layer\n",
    "# Note: Parameters of newly constructed modules have requires_grad=True by default\n",
    "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "# Move to desired device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpjZOVaF6iMf"
   },
   "outputs": [],
   "source": [
    "# Check number of trainable parameters\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUZZ2CB52rUy"
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBNLtqobKcuI"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = train_model(\n",
    "    model, criterion, \n",
    "    optimizer_conv, exp_lr_scheduler, \n",
    "    num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3DEgSN4Kmma"
   },
   "outputs": [],
   "source": [
    "visualize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byF99tMEEtv2"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(dataloaders['test'], model, criterion)\n",
    "print(f\"Test: loss={test_loss:>8f}, acc={(100*test_acc):>0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "978XBalxJbSd"
   },
   "source": [
    "## Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Sek8-6y4jW9"
   },
   "outputs": [],
   "source": [
    "layers = list(model.children())\n",
    "for li, l in enumerate(layers):\n",
    "    print(f\"{li}: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUlui4Iz4hmo"
   },
   "outputs": [],
   "source": [
    "# Determine how many layers to freeze\n",
    "fine_tune_at = 7\n",
    "ct = 0\n",
    "for child in model.children():\n",
    "    ct += 1\n",
    "    if ct < 7:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LaBkM8n_6lWt"
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-gTGkYGI7DV"
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "# Note: we typically use a lower learning rate for model fine-tuning\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8EYfLXjJKx7"
   },
   "outputs": [],
   "source": [
    "model = train_model(\n",
    "    model, criterion, \n",
    "    optimizer_ft, exp_lr_scheduler,\n",
    "    num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYmuPxTeJSAN"
   },
   "outputs": [],
   "source": [
    "visualize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4I0lm788Fsvu"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(dataloaders['test'], model, criterion)\n",
    "print(f\"Test: loss={test_loss:>8f}, acc={(100*test_acc):>0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bry9ssFx9P6E"
   },
   "source": [
    "# Exercise: Create your own custom dataset\n",
    "\n",
    "You need to create a new custom dataset for image classification whose datasets can be easily obtained from the Internet. Your dataset **MUST** have at least 4 classes. Then use this notebook to do transfer learning. You may want to adjust the code as appropriate:\n",
    "* Change the model (e.g., EfficientNet, Inception, etc.) [[link](https://pytorch.org/vision/stable/models.html#table-of-all-available-classification-weights)]\n",
    "* Change the training parameters (e.g., batch_size, learning rate, epoch, etc.)\n",
    "* Change the number of layer to be frozen.\n",
    "* and so on.\n",
    "\n",
    "Please make sure that you have enough training (>=15), valid (>=5) and test (>=5) examples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0ehVXHa-KKH"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
